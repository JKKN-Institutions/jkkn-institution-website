# Multi-Institution robots.txt Configuration

This directory contains the configuration for institution-specific `robots.txt` files.

## How It Works

The project uses a dynamic route handler (`app/robots.txt/route.ts`) that serves different `robots.txt` content based on the `NEXT_PUBLIC_INSTITUTION_ID` environment variable.

### Files

- **`robots-txt.config.ts`** - Configuration file containing robots.txt content for all institutions
- **`app/robots.txt/route.ts`** - Route handler that serves the correct robots.txt
- **`app/robots.ts`** - Legacy file (kept for reference, not actively used)

## Current Status

| Institution | Status | Details |
|-------------|--------|---------|
| Engineering | ‚úÖ Complete | Full robots.txt with 30+ AI crawlers, detailed blocking rules |
| Main | üìù Basic | Simple placeholder, uses legacy robots.ts as reference |
| Dental | üìù Basic | Simple placeholder, needs customization |
| Pharmacy | üìù Basic | Simple placeholder, needs customization |
| Arts & Science | üìù Basic | Simple placeholder, needs customization |
| Nursing | üìù Basic | Simple placeholder, needs customization |

## How to Add/Update robots.txt for an Institution

### Step 1: Edit `robots-txt.config.ts`

Find the function for your institution (e.g., `getDentalRobotsTxt`) and update the content:

```typescript
function getDentalRobotsTxt(siteUrl: string): string {
  return `# robots.txt ‚Äî JKKN Dental College
# Website: ${siteUrl}
# Last Updated: 2026-02-16

User-agent: *
Allow: /
Disallow: /_next/
Disallow: /api/
Disallow: /admin/

# Add your custom rules here...

Sitemap: ${siteUrl}/sitemap.xml
Host: ${siteUrl}
`
}
```

### Step 2: Test Locally

1. Switch to the institution environment:
   ```bash
   npm run dev:dental
   # or
   npm run dev:pharmacy
   # etc.
   ```

2. Visit `http://localhost:3000/robots.txt` in your browser

3. Verify the correct robots.txt content is displayed

### Step 3: Deploy

Push your changes to GitHub. Vercel will automatically deploy to all institution sites.

## Engineering College robots.txt Features

The Engineering College robots.txt includes:

- ‚úÖ **50+ bot-specific directives**
- ‚úÖ **80+ blocking rules** for admin paths, duplicate content, query parameters
- ‚úÖ **30+ AI crawlers** explicitly allowed (ChatGPT, Claude, Gemini, Perplexity, etc.)
- ‚úÖ **Social media crawlers** for link previews
- ‚úÖ **SEO tool crawlers** with rate limiting
- ‚úÖ **Spam/scraper bots** blocked
- ‚úÖ **Detailed comments** explaining each rule

### AI Crawler Strategy (Engineering)

The Engineering robots.txt follows an **"ALLOW ALL AI CRAWLERS"** strategy to maximize visibility in:

- Google AI Overviews (40%+ of searches show AI answers)
- ChatGPT Search (fastest growing search alternative)
- Perplexity AI (top AI-native search)
- Claude AI (professional/researcher usage)
- Grok (X/Twitter integration)
- Meta AI (500M+ WhatsApp/Instagram users)
- Apple Intelligence (every iOS device)
- Microsoft Copilot (enterprise/education)

**Rationale:** For educational institutions competing for student admissions, AI visibility is now as critical as Google page 1 ranking.

## Using Engineering robots.txt as a Template

To apply the Engineering College strategy to other institutions:

1. Copy the `getEngineeringRobotsTxt()` function content
2. Paste into the target institution's function (e.g., `getDentalRobotsTxt()`)
3. Update the header comment with the institution name
4. Adjust any institution-specific paths if needed

Example:

```typescript
function getDentalRobotsTxt(siteUrl: string): string {
  // Copy from getEngineeringRobotsTxt() and modify header:
  return `# =============================================================================
# robots.txt ‚Äî JKKN Dental College  ‚Üê Change this
# Website: ${siteUrl}
# Framework: Next.js (App Router)
# Last Updated: 2026-02-16
# Version: 2.0 (Improved ‚Äî expanded AI coverage + detailed blocking)
# =============================================================================
...rest of the content...
`
}
```

## Environment Variables

The robots.txt system uses these environment variables:

- `NEXT_PUBLIC_INSTITUTION_ID` - Determines which robots.txt to serve
  - Values: `main`, `engineering`, `dental`, `pharmacy`, `arts-science`, `nursing`
- `NEXT_PUBLIC_SITE_URL` - Used in sitemap URL and Host directive
  - Example: `https://engg.jkkn.ac.in`

These are configured in:
- Local: `.env.local` (generated by institution switcher)
- Production: Vercel project environment variables

## Caching

The robots.txt route is:
- **Static** - Built at deploy time (`force-static`)
- **Cached** - 24 hours (`revalidate: 86400`)
- **CDN Cached** - Max-age 24 hours

This ensures fast delivery and minimal server load.

## Verification

### Check robots.txt in Production

Visit `https://[institution-domain]/robots.txt`:

- Main: https://jkkn.ac.in/robots.txt
- Engineering: https://engg.jkkn.ac.in/robots.txt
- Dental: https://dental.jkkn.ac.in/robots.txt
- Pharmacy: https://pharmacy.jkkn.ac.in/robots.txt

### Google Search Console

After deploying, submit robots.txt in Google Search Console:

1. Go to Search Console for the institution
2. Navigate to "robots.txt Tester"
3. Test the file
4. Submit for indexing

## Troubleshooting

### Wrong robots.txt being served

**Problem:** Engineering robots.txt shows on Dental site

**Solution:** Check environment variables in Vercel:
1. Go to Vercel project settings
2. Verify `NEXT_PUBLIC_INSTITUTION_ID` is set correctly
3. Redeploy if needed

### robots.txt not updating

**Problem:** Changes not visible after deploy

**Solution:**
1. Clear CDN cache (Vercel auto-purges on deploy)
2. Wait 24 hours for cache expiry
3. Hard refresh in browser (Ctrl+Shift+R)

### 404 on robots.txt

**Problem:** /robots.txt returns 404

**Solution:**
1. Verify `app/robots.txt/route.ts` exists
2. Check build logs for errors
3. Ensure `getRobotsTxt()` function exists

## Best Practices

1. **Keep comments** - They provide valuable context for future maintainers
2. **Test locally first** - Always test with institution switcher before deploying
3. **Use Engineering as template** - It's production-tested and comprehensive
4. **Block admin paths** - Always disallow `/admin/`, `/api/`, `/_next/`
5. **Allow AI crawlers** - Maximize visibility in AI search results
6. **Use crawl-delay** - For SEO tools to prevent server overload
7. **Include sitemap** - Always reference sitemap.xml

## Related Documentation

- [Multi-Institution Architecture](../../docs/MULTI-INSTITUTION-ARCHITECTURE.md)
- [Institution Switcher Guide](../../scripts/switch-institution.ts)
- [SEO Strategy](../../docs/MULTI-TENANT-SEO.md)
